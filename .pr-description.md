# Documentation: job streams, partial results, and recent changes

This PR updates the docs to reflect the job-stream and performance work from the last couple of days. No code changes—documentation only.

## What changed in the product (what the docs now describe)

**Job streams show partial results.** When you open the job stream (`GET /api/v1/jobs/:jobId/stream`), you no longer only get progress and a final “complete” event. You get usable results as each stage finishes:

- **Content generation:** Context loaded, then the full blog post (so the user can read while visuals and SEO run), then visual suggestions, then SEO analysis, then complete. Existing clients that only care about progress and complete keep working.
- **Website analysis:** Page scrape (title, meta, headings), then the analysis (org summary, CTAs), then audiences, then pitches, then images—with optional per-item events so you can show each audience card, pitch, or image as it arrives. Progress steps are finer-grained (e.g. “Researching business”, “Researching keywords & SEO”), and there’s a dedicated “scrape-phase” event so you can show a step-by-step log during “Fetching page content”.

**Stream pacing and auth.** Events are sent one per tick so the frontend doesn’t get bursts. Job access is stricter: if the job is tied to a JWT user who isn’t in the users table, the API returns 401. When the queue sees a JWT but no matching user in the DB, it falls back to session-only so anonymous/session jobs still run and can be streamed with `?sessionId=`.

**Website analysis speed and cache.** Research used to run as two sequential calls (business research, then keyword research); it now runs in parallel, so the research phase is roughly the longer of the two (~15–20s typical) instead of their sum (~30–40s). Results are cached by URL for **30 days**; a repeat analysis of the same site returns the cached result immediately (no scrape, analysis, audiences, pitches, or images). Cache is keyed by URL only. The existing-audiences DB query is started early and overlaps with scrape + analyze, so it no longer adds its full duration to the critical path.

**Content generation speed.** After the blog body is ready, visual suggestions and SEO analysis run in parallel (`Promise.all`). The time to complete after the blog is therefore the **max** of the two steps (e.g. ~20–30s) instead of their **sum** (e.g. ~40–60s). Same API shape; jobs just finish sooner.

## What changed in this PR (docs only)

- **RECENT_UPDATES.md** — Added sections for job-stream partial results, stream/auth fixes, website-analysis speed and caching, and content-generation parallel visuals/SEO. Updated the intro and quick-reference table so the doc reflects both the original guardrails (CI, migration validation, smoke test, Vercel) and the new streaming/perf work.
- **frontend-job-queue-handoff.md** — At the top, added a short note that the job stream is an option (with links to the content-generation and website-analysis stream handoffs). In the Reference section, added links to the stream docs and the job-stream event schema.

Existing handoff docs ([content-generation-stream-frontend-handoff.md](docs/content-generation-stream-frontend-handoff.md), [website-analysis-stream-frontend-handoff.md](docs/website-analysis-stream-frontend-handoff.md), [job-stream-sse.md](docs/job-stream-sse.md)) were already added in earlier commits; this PR only ties them into RECENT_UPDATES and the main job-queue handoff so the frontend team has one place to see what’s new and where to look for stream contracts.
